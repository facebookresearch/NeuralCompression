{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82047528",
   "metadata": {},
   "source": [
    "# NeuralCompression Flop Counter Example\n",
    "\n",
    "Welcome! In this notebook we'll walkthrough using `neuralcompression`'s flop counter to calculate the computational complexity of a compression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a69e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from neuralcompression.functional import count_flops\n",
    "from neuralcompression.models import ScaleHyperprior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a539d",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "To get started with the flop counter, simply instantiate the model you want to evaluate and pass it to `neuralcompression.functional.count_flops`, along with the inputs it should be evaluated on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc3f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size = 3, padding = 1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, stride = 2, kernel_size = 5, padding = 2),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 16 * 16, 10)\n",
    ")\n",
    "\n",
    "inputs = (torch.randn(5, 3, 32, 32),)\n",
    "\n",
    "results = count_flops(model, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91755cbc",
   "metadata": {},
   "source": [
    "The result returned by the flop counter is a 3-tuple. The first element records the total number of flops performed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be3fb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19619840"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81126c0",
   "metadata": {},
   "source": [
    "The second element in the return tuple breaks down this count by operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c3aa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv': 18595840, 'batch_norm': 614400, 'linear': 409600}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2903f4",
   "metadata": {},
   "source": [
    "The third item returned by the counter records all the operations that the counter didn't know how to count flops for (more detail about these ops below). This dicitionary (or more specifically a [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)) maps unknown op names to the number of times those ops were called in the model.\n",
    "\n",
    "In our example, all of the model's operations are supported by the counter, so this dictionary is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb21bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ea68b",
   "metadata": {},
   "source": [
    "In general, the majority of common ML operations are already supported, although some operations' counts may be approximations (e.g. one floating point addition and one square root are both counted as 1 flop). Much more complicated models than the toy example above can be evaluated without accumulating lots of unsupported operations, such as the [scale hyperprior model](https://arxiv.org/abs/1802.01436):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961a052c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigger_model = ScaleHyperprior(network_channels=32, compression_channels=64)\n",
    "_,_, unsupported_ops = count_flops(bigger_model, (torch.randn(1, 3, 64, 64),))\n",
    "\n",
    "len(unsupported_ops) == 0 # Verifying no unsupported operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce26204",
   "metadata": {},
   "source": [
    "However, if you need to add support for more ops or override the counter's default implementation, read on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f36115",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "### How the Counter Works\n",
    "\n",
    "The flop counter in `neuralcompression` makes heavy use of the counting utilities in [fvcore](https://github.com/facebookresearch/fvcore). Counting a model's flops is a two-step process:\n",
    "\n",
    "1. Using PyTorch's [TorchScript](https://pytorch.org/docs/stable/jit.html) capabilities, the model is first JIT-traced into a computational graph. Each node in the graph corresponds to an ATen (linear algebra) operation, like matrix multiplications, convolutions, and elementwise operations like additions/subtractions.\n",
    "\n",
    "\n",
    "2. Every node in the traced graph is iterated over, and if a node is associated with a registered flop-counting function, that function is invoked and the counted flops are added to the model's total.\n",
    "\n",
    "\n",
    "In this second step, the counter's registered flop functions are stored as a dictionary mapping operator names (e.g. `aten::add`, `aten::matmul`) to counter functions. These functions have a signature of:\n",
    "\n",
    "```\n",
    "def my_counter_function(inputs: List[torch._C.Value], outputs: List[torch._C.Value]) -> float\n",
    "```\n",
    "\n",
    "where objects of type `torch._C.Value` represent the symbolic inputs and outputs for the node in the graph (i.e. they're not actual concrete `Tensor`s). Check out the `fvcore` codebase for [examples of how to write counter functions](https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/jit_handles.py).\n",
    "\n",
    "### Customizing the Counter\n",
    "\n",
    "To register additional counter functions or override the counter's default implementation for specific operations, use the `counter_overrides` argument, which takes the form of a dictionary mapping operator names to the corresponding counter functions you wish to use. As an example, consider the following simple module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa3b8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def forward(self,x,y,z):\n",
    "        return x + y * z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb5c1db",
   "metadata": {},
   "source": [
    "By default, the elementwise addition or multiplication of two tensors of shape `N x M` will contribute `N x M` flops to the total computational complexity of a model (since each scalar addition/multiplication is counted as 1 flop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b78a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "M = 32\n",
    "\n",
    "inp = torch.randn(N, M)\n",
    "\n",
    "flops,_,_ = count_flops(MyModule(), (inp, inp, inp))\n",
    "flops == 2 * N * M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d5521",
   "metadata": {},
   "source": [
    "However, let's say that you wanted to ignore all the flops coming from addition operations. You could do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7a806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_flops, _, _ = count_flops(\n",
    "    MyModule(), \n",
    "    (inp, inp, inp), \n",
    "    counter_overrides = {\n",
    "        \"aten::add\": lambda inps,outs: 0.0\n",
    "    }\n",
    ")\n",
    "\n",
    "new_flops == N * M # only the flops from multiplications are counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05767f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
